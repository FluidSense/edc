{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"medical.jpg\" width=\"400\" height=\"200\" style=\"float: left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial, we show you how to compute counterfactual explanations for explaining positively-predicted instances. We use textual data (20newsgroups) where the goal is to predict whether a document is about a 'Medical' topic. The counterfactual explanation shows a set of words such that, when removing them from the document, the predicted topic is not longer 'Medical'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries and import data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sedc_algorithm\n",
    "from function_edc import fn_1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run sedc_algorithm.py #run sedc_algorithm.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this tutorial, we will use the 20newsgroups data set. For simplicity, we will use a binary target variable: medical topic vs non-medical topic (sci.med).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, we preprocess the raw textual data into a structured data format that can be used for modelling. We lowercase all words in the documents, remove stopwords and lemmatize the textual data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_data = newsgroups.data\n",
    "### Lowercase (normalization) ###\n",
    "data_=[]\n",
    "for story in newsgroups_data:\n",
    "    new=story.lower()\n",
    "    data_.append(new)\n",
    "\n",
    "### Remove stopwords ###\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "newsgroups_dataset=[]\n",
    "for story in data_:\n",
    "    words=word_tokenize(story)\n",
    "    text=\"\"\n",
    "    for words in word_tokenize(story):\n",
    "        if not words in stop_words:\n",
    "            text+=(\" \"+words)\n",
    "    newsgroups_dataset.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_dataset_2=[]\n",
    "for story in data_:\n",
    "    words=word_tokenize(story)\n",
    "    text=\"\"\n",
    "    for words in word_tokenize(story):\n",
    "        text+=(\" \"+words)\n",
    "    newsgroups_dataset_2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lemmatizer modules.\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "newsgroups_lemma=[]\n",
    "for story in newsgroups_dataset:\n",
    "    words=word_tokenize(story)\n",
    "    text=\"\"\n",
    "    for words in word_tokenize(story):\n",
    "        lemma_word=lemmatizer.lemmatize(words)\n",
    "        extra=\" \"+str(lemma_word)\n",
    "        text+=extra\n",
    "    newsgroups_lemma.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We create a vectorizer object to transform the preprocessed raw data (removed stop words, converted to lowercase, lemmatized) into a term frequency-inverse document frequency format (td-idf).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data into a training and test set (80-20%).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for random_state = 0\n",
    "indices=np.arange(18846)\n",
    "from sklearn.model_selection import train_test_split\n",
    "indices_train, indices_test = train_test_split(indices, test_size=0.2, random_state=0)\n",
    "indices_train, indices_val = train_test_split(indices_train, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data splits from preprocessed textual data #\n",
    "newsgroups_lemma_train = list(newsgroups_lemma[i] for i in indices_train)\n",
    "newsgroups_lemma_test = list(newsgroups_lemma[i] for i in indices_test)\n",
    "newsgroups_lemma_val=list(newsgroups_lemma[i] for i in indices_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the vectorizer on the training data. Transform the data of training, validation and test data using this vectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.fit_transform(newsgroups_lemma_train)\n",
    "x_test = vectorizer.transform(newsgroups_lemma_test)\n",
    "x_val = vectorizer.transform(newsgroups_lemma_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the target variable (1 refers to a medical topic, 0 to another topic).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = newsgroups.target\n",
    "Y = np.reshape(Y,(np.size(Y),1))\n",
    "# Topic: sci.med\n",
    "y = Y.copy()\n",
    "for i in range(len(Y)):\n",
    "    if (Y[i]==13):\n",
    "        y[i]=1\n",
    "    else: y[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[indices_train]\n",
    "y_test = y[indices_test]\n",
    "y_val = y[indices_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use a Support Vector Machine model with a linear kernel. We finetune the regularization parameter using a hold-out validation data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balance of target in training subset is 0.053507.\n",
      "The finetuning process has ended...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = [10**(-3),10**(-2),10**(-1),10**(0),10**(1),10**(2)]\n",
    "p = np.sum(y_train)/np.size(y_train)\n",
    "print(\"The balance of target in training subset is %f.\" %p)\n",
    "#There are about 5% documents having a 'Medical' topic in the training data.\n",
    "\n",
    "accuracy_vals=[]\n",
    "for c in C:\n",
    "    SVC_model = SVC(C = c, kernel=\"linear\", probability=True)\n",
    "    SVC_model.fit(x_train, y_train)\n",
    "        \n",
    "    probs = SVC_model.decision_function(x_val)\n",
    "    threshold_classifier_probs = np.percentile(probs,(100-(p*100)))\n",
    "    predictions_probs = (probs >= threshold_classifier_probs) #Explicit, discrete predictions for validation data instances\n",
    "                \n",
    "    accuracy_val = accuracy_score(y_val, np.array(predictions_probs))\n",
    "    accuracy_vals.append(accuracy_val)\n",
    "print(\"The finetuning process has ended...\")\n",
    "    \n",
    "C_optimal_accuracy = C[np.argmax(accuracy_vals)]\n",
    "SVC_best = SVC(C = C_optimal_accuracy, kernel=\"linear\", probability=True)\n",
    "SVC_best.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model on the test data is 0.979045\n"
     ]
    }
   ],
   "source": [
    "probs = SVC_best.decision_function(x_test)\n",
    "threshold_classifier_probs = np.percentile(probs,(100-(p*100)))\n",
    "predictions_probs = (probs >= threshold_classifier_probs) #Explicit, discrete predictions for validation data instances\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, np.array(predictions_probs))\n",
    "print(\"The accuracy of the model on the test data is %f\" %accuracy_test)\n",
    "\n",
    "indices_probs_pos = np.nonzero(predictions_probs)#Indices of the test documents that are positively-predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = SVC_best \n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "def classifier_fn(X):\n",
    "    c=classification_model.decision_function(X)\n",
    "    y_predicted_proba = c\n",
    "    return y_predicted_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an SEDC explainer object. By default, the SEDC algorithm stops looking for explanations when a first explanation is found or when a 5-minute time limit is exceeded or when more than 50 iterations are required (see edc_agnostic.py for more details). Only the active (nonzero) features are perturbed (set to zero) to evaluate the impact on the model's predicted output. In other words, only the movies that a user has watched can become part of the counterfactual explanation of the model prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_SEDC = SEDC_Explainer(feature_names = feature_names, \n",
    "                               threshold_classifier = threshold_classifier_probs, \n",
    "                               classifier_fn = classifier_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show indices of positively-predicted test instances.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  17,   21,   41,   73,  143,  161,  165,  183,  225,  228,  232,\n",
       "         236,  267,  273,  298,  365,  418,  439,  475,  482,  506,  517,\n",
       "         523,  552,  557,  567,  583,  586,  609,  632,  638,  643,  657,\n",
       "         660,  662,  669,  682,  687,  694,  705,  744,  764,  772,  804,\n",
       "         817,  834,  861,  893,  896,  897,  913,  948,  964, 1008, 1009,\n",
       "        1043, 1045, 1071, 1074, 1147, 1151, 1156, 1186, 1198, 1216, 1222,\n",
       "        1246, 1248, 1253, 1288, 1291, 1329, 1360, 1394, 1411, 1457, 1477,\n",
       "        1496, 1538, 1539, 1549, 1577, 1624, 1625, 1628, 1633, 1698, 1735,\n",
       "        1740, 1752, 1766, 1791, 1794, 1811, 1839, 1928, 1957, 1961, 1988,\n",
       "        2004, 2020, 2046, 2092, 2098, 2107, 2126, 2139, 2141, 2143, 2146,\n",
       "        2153, 2199, 2210, 2222, 2248, 2260, 2270, 2274, 2313, 2379, 2386,\n",
       "        2389, 2390, 2426, 2434, 2480, 2492, 2501, 2504, 2522, 2535, 2554,\n",
       "        2557, 2617, 2620, 2707, 2752, 2761, 2772, 2815, 2836, 2876, 2905,\n",
       "        2931, 2944, 2967, 2981, 3016, 3025, 3055, 3058, 3080, 3086, 3116,\n",
       "        3117, 3157, 3161, 3186, 3196, 3217, 3228, 3251, 3266, 3278, 3279,\n",
       "        3282, 3290, 3292, 3315, 3319, 3320, 3368, 3371, 3389, 3395, 3404,\n",
       "        3414, 3424, 3448, 3468, 3472, 3475, 3481, 3498, 3506, 3515, 3545,\n",
       "        3549, 3553, 3569, 3579, 3624, 3644, 3645, 3684, 3702, 3716, 3737,\n",
       "        3750, 3759, 3767, 3769], dtype=int64),)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_probs_pos #all documents that have a predicted 'Medical' topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain why the document with index = 143 is predicted as a 'Medical' topic by the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = list(newsgroups_dataset_2[i] for i in indices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The document looks as follows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" -allergy medicine , huh ? is this just to get rid of the resultant migraine or whatever , or does it actually suppress allergic reactions ? ( i.e . like an antihistamine does ? ) as far as doctors over here are concerned , if you slip up and eat something you 're allergic to ( even if they wo n't test you to tell you what to avoid ) then tough ; if a _cheap_ medicine will alleviate your symptoms , then fine , otherwise you just suffer . one doctor did prescribe me imigran ( costs the nhs # 48 for 6 tablets ) after having to rehydrate me because i 'd been throwing up for four solid days and could n't even drink water - but i got taken off it again when i moved and had to change doctors . reasoning : they did not know what the side-effects were because it was new . ok , fine - but it has passed the safety tests to get on the prescription list , and anyway i was prepared to take the risk to have quality of life now . the only alternatives i have is to get it prescribed privately , which i can not afford , or to pay a private allergy specialist to test me and tell me what to avoid . i am fairly certain i am allergic to more than one chemical additive , as a lot of things i ca n't eat have nothing in common except things i know are safe , so testing myself is n't really an option ; there are too many permutations .\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test[73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization is complete.\n",
      "\n",
      " Elapsed time 0 \n",
      "\n",
      "\n",
      " Iteration 1 \n",
      "\n",
      "The difference is 0.002621\n",
      "Index is 20.000000\n",
      "Length of new_combinations is 1 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.003974\n",
      "\n",
      " Elapsed time 0 \n",
      "\n",
      "\n",
      " Size combis to expand 174 \n",
      "\n",
      "\n",
      " Iteration 2 \n",
      "\n",
      "The difference is 0.003974\n",
      "Index is 37.000000\n",
      "Length of new_combinations is 2 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.004851\n",
      "\n",
      " Elapsed time 0 \n",
      "\n",
      "\n",
      " Size combis to expand 259 \n",
      "\n",
      "\n",
      " Iteration 3 \n",
      "\n",
      "The difference is 0.004851\n",
      "Index is 7.000000\n",
      "Length of new_combinations is 3 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.005633\n",
      "\n",
      " Elapsed time 1 \n",
      "\n",
      "\n",
      " Size combis to expand 343 \n",
      "\n",
      "\n",
      " Iteration 4 \n",
      "\n",
      "The difference is 0.005633\n",
      "Index is 34.000000\n",
      "Length of new_combinations is 4 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.006071\n",
      "\n",
      " Elapsed time 2 \n",
      "\n",
      "\n",
      " Size combis to expand 426 \n",
      "\n",
      "\n",
      " Iteration 5 \n",
      "\n",
      "The difference is 0.006071\n",
      "Index is 73.000000\n",
      "Length of new_combinations is 5 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.006428\n",
      "\n",
      " Elapsed time 2 \n",
      "\n",
      "\n",
      " Size combis to expand 508 \n",
      "\n",
      "\n",
      " Iteration 6 \n",
      "\n",
      "The difference is 0.006428\n",
      "Index is 1.000000\n",
      "Length of new_combinations is 6 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.006750\n",
      "\n",
      " Elapsed time 3 \n",
      "\n",
      "\n",
      " Size combis to expand 589 \n",
      "\n",
      "\n",
      " Iteration 7 \n",
      "\n",
      "The difference is 0.006750\n",
      "Index is 1.000000\n",
      "Length of new_combinations is 7 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.006994\n",
      "\n",
      " Elapsed time 4 \n",
      "\n",
      "\n",
      " Size combis to expand 669 \n",
      "\n",
      "\n",
      " Iteration 8 \n",
      "\n",
      "The difference is 0.006994\n",
      "Index is 15.000000\n",
      "Length of new_combinations is 8 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.007166\n",
      "\n",
      " Elapsed time 5 \n",
      "\n",
      "\n",
      " Size combis to expand 748 \n",
      "\n",
      "\n",
      " Iteration 9 \n",
      "\n",
      "The difference is 0.007166\n",
      "Index is 75.000000\n",
      "Length of new_combinations is 9 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.007330\n",
      "\n",
      " Elapsed time 7 \n",
      "\n",
      "\n",
      " Size combis to expand 826 \n",
      "\n",
      "\n",
      " Iteration 10 \n",
      "\n",
      "The difference is 0.007330\n",
      "Index is 52.000000\n",
      "Length of new_combinations is 10 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.007491\n",
      "\n",
      " Elapsed time 8 \n",
      "\n",
      "\n",
      " Size combis to expand 903 \n",
      "\n",
      "\n",
      " Iteration 11 \n",
      "\n",
      "The difference is 0.007491\n",
      "Index is 41.000000\n",
      "Length of new_combinations is 11 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.007647\n",
      "\n",
      " Elapsed time 10 \n",
      "\n",
      "\n",
      " Size combis to expand 979 \n",
      "\n",
      "\n",
      " Iteration 12 \n",
      "\n",
      "The difference is 0.007647\n",
      "Index is 58.000000\n",
      "Length of new_combinations is 12 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.007803\n",
      "\n",
      " Elapsed time 11 \n",
      "\n",
      "\n",
      " Size combis to expand 1054 \n",
      "\n",
      "\n",
      " Iteration 13 \n",
      "\n",
      "The difference is 0.007803\n",
      "Index is 58.000000\n",
      "Length of new_combinations is 13 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.007931\n",
      "\n",
      " Elapsed time 13 \n",
      "\n",
      "\n",
      " Size combis to expand 1128 \n",
      "\n",
      "\n",
      " Iteration 14 \n",
      "\n",
      "The difference is 0.007931\n",
      "Index is 38.000000\n",
      "Length of new_combinations is 14 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.008049\n",
      "\n",
      " Elapsed time 15 \n",
      "\n",
      "\n",
      " Size combis to expand 1201 \n",
      "\n",
      "\n",
      " Iteration 15 \n",
      "\n",
      "The difference is 0.008049\n",
      "Index is 5.000000\n",
      "Length of new_combinations is 15 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.008166\n",
      "\n",
      " Elapsed time 17 \n",
      "\n",
      "\n",
      " Size combis to expand 1273 \n",
      "\n",
      "\n",
      " Iteration 16 \n",
      "\n",
      "The difference is 0.008166\n",
      "Index is 64.000000\n",
      "Length of new_combinations is 16 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.008191\n",
      "\n",
      " Elapsed time 19 \n",
      "\n",
      "\n",
      " Size combis to expand 1344 \n",
      "\n",
      "\n",
      " Iteration 17 \n",
      "\n",
      "The difference is 0.008191\n",
      "Index is 45.000000\n",
      "Length of new_combinations is 17 features.\n",
      "New combination cannot be expanded\n",
      "\n",
      " Elapsed time 19 \n",
      "\n",
      "\n",
      " Size combis to expand 1344 \n",
      "\n",
      "Iterations are done.\n",
      "\n",
      " Elapsed time 19 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 73\n",
    "instance_idx = x_test[index]\n",
    "explanation = explainer_SEDC.explanation(instance_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The explanation contains 17 words out of the 88 featurized words that are used by the SVM model.**\n",
    "\n",
    "**Show more information about the explanation(s): *explanation[0]* shows the explanation set(s), *explanation[1]* shows the number of active features of the instance to explain, *explanation[2]* shows the number of explanations found, *explanation[3]* shows the number of features in the smallest-sized explanation, *explanation[4]* shows the time elapsed in seconds to find the explanation, *explanation[5]* shows the predicted score change when removing the feature(s) in the smallest-sized explanation, *explanation[6]* shows the number of iterations that the algorithm needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['doctor',\n",
       "   'allergic',\n",
       "   'allergy',\n",
       "   'medicine',\n",
       "   'migraine',\n",
       "   'symptom',\n",
       "   'antihistamine',\n",
       "   'eat',\n",
       "   'additive',\n",
       "   'avoid',\n",
       "   'reaction',\n",
       "   'risk',\n",
       "   'prescribed',\n",
       "   'prescription',\n",
       "   'tablet',\n",
       "   'water',\n",
       "   'chemical']],\n",
       " 88,\n",
       " 26,\n",
       " 17,\n",
       " 19.83780312538147,\n",
       " [array([0.00826831])],\n",
       " 17)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF the document did not contain the word(s) ['doctor', 'allergic', 'allergy', 'medicine', 'migraine', 'symptom', 'antihistamine', 'eat', 'additive', 'avoid', 'reaction', 'risk', 'prescribed', 'prescription', 'tablet', 'water', 'chemical'], THEN the predicted topic would no longer be 'Medical'.\n"
     ]
    }
   ],
   "source": [
    "print(\"IF the document did not contain the word(s) \" + str(explanation[0][0]) + \", THEN the predicted topic would no longer be 'Medical'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain why the document with index = 143 is predicted as a 'Medical' topic by the model.**\n",
    "\n",
    "**The document looks as follows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" the burden of proof rests upon those who claim the existence of this `` syndrome '' . to date , these claims are unsubstantiated by any available data . hopefully , as a scientist , you would take issue with anyone overstating their conclusions based upon their data . gee , i have many interesting and enlightening anecdotes about myself , my friends , and my family , but in the practice of medicine i expect and demand more rigorous rationales for basing therapy than `` aunt susie 's brother-in-law ... '' . anecdotal evidence may provide inspiration for a hypothesis , but rarely proves anything in a positive sense . and unlike mathematics , boolean logic rarely applies directly to medical issues , and so evidence of 'exceptions ' does not usually disprove but rather modifies current concepts of disease . i would characterize it not as 'abject disbelief ' but rather 'scientific outrage over vastly overstated conclusions ' . i have no problem with such an approach ; but this is not what is happening in the 'trenches ' of this diagnosis .\""
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test[165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization is complete.\n",
      "\n",
      " Elapsed time 0 \n",
      "\n",
      "\n",
      " Iteration 1 \n",
      "\n",
      "The difference is 0.000730\n",
      "Index is 24.000000\n",
      "Length of new_combinations is 1 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.001289\n",
      "\n",
      " Elapsed time 0 \n",
      "\n",
      "\n",
      " Size combis to expand 144 \n",
      "\n",
      "\n",
      " Iteration 2 \n",
      "\n",
      "The difference is 0.001289\n",
      "Index is 5.000000\n",
      "Length of new_combinations is 2 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.001740\n",
      "\n",
      " Elapsed time 0 \n",
      "\n",
      "\n",
      " Size combis to expand 214 \n",
      "\n",
      "\n",
      " Iteration 3 \n",
      "\n",
      "The difference is 0.001740\n",
      "Index is 47.000000\n",
      "Length of new_combinations is 3 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.002149\n",
      "\n",
      " Elapsed time 1 \n",
      "\n",
      "\n",
      " Size combis to expand 283 \n",
      "\n",
      "\n",
      " Iteration 4 \n",
      "\n",
      "The difference is 0.002149\n",
      "Index is 66.000000\n",
      "Length of new_combinations is 4 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.002444\n",
      "\n",
      " Elapsed time 1 \n",
      "\n",
      "\n",
      " Size combis to expand 351 \n",
      "\n",
      "\n",
      " Iteration 5 \n",
      "\n",
      "The difference is 0.002444\n",
      "Index is 34.000000\n",
      "Length of new_combinations is 5 features.\n",
      "New combinations can be expanded\n",
      "Threshold is 0.002546\n",
      "\n",
      " Elapsed time 2 \n",
      "\n",
      "\n",
      " Size combis to expand 418 \n",
      "\n",
      "\n",
      " Iteration 6 \n",
      "\n",
      "The difference is 0.002546\n",
      "Index is 11.000000\n",
      "Length of new_combinations is 6 features.\n",
      "New combination cannot be expanded\n",
      "\n",
      " Elapsed time 2 \n",
      "\n",
      "\n",
      " Size combis to expand 418 \n",
      "\n",
      "Iterations are done.\n",
      "\n",
      " Elapsed time 2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 165\n",
    "instance_idx = x_test[index]\n",
    "explanation = explainer_SEDC.explanation(instance_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF the document did not contain the word(s) ['disease', 'medical', 'medicine', 'syndrome', 'therapy', 'diagnosis'], THEN the predicted topic would no longer be 'Medical'.\n"
     ]
    }
   ],
   "source": [
    "print(\"IF the document did not contain the word(s) \" + str(explanation[0][0]) + \", THEN the predicted topic would no longer be 'Medical'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['disease', 'medical', 'medicine', 'syndrome', 'therapy', 'diagnosis']],\n",
       " 73,\n",
       " 5,\n",
       " 6,\n",
       " 2.3969829082489014,\n",
       " [array([0.00273369])],\n",
       " 6)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
